{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalSlowakiewicz/Machine-Learning/blob/master/Homework6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment: Understanding Splitting Criteria in CART for Regression**\n",
        "---------------------\n",
        "\n",
        "In this assignment, you will explore three common formulations of the splitting criterion used in **CART (Classification and Regression Trees)** for **regression problems**:\n",
        "\n",
        "1. **Local RSS Minimization**  \n",
        "2. **RSS Gain Maximization**  \n",
        "3. **Total RSS Minimization**\n",
        "\n",
        "You will investigate whether any of these criteria are equivalent, and you will design an experiment to determine which criterion is actually employed in a standard implementation such as **scikit-learn’s DecisionTreeRegressor**.\n",
        "\n",
        "\n",
        "\n",
        "## **The Problem**\n",
        "\n",
        "Many treatments of CART for regression describe the split selection process in different ways. Below are three frequently cited formulations. Suppose we have a dataset with features $X$ and target $y$, and we seek to choose a feature $X_j$ and a threshold $t$ to split the data into two regions $R_1(X_j, t)$ and $R_2(X_j, t)$. Denote by $\\bar{y}_{R_m}$ the mean of targets within region $R_m$.\n",
        "\n",
        "1. **Local RSS Minimization**  \n",
        "   We select the feature and threshold that minimize the **sum of squared errors** in the two resulting child nodes:\n",
        "   $$\n",
        "   (X_j^*, t^*) = \\arg\\min_{X_j, t} \\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2.\n",
        "   $$\n",
        "\n",
        "2. **RSS Gain Maximization**  \n",
        "\n",
        "   It is also a local method, looking only at a parent and two child nodes.\n",
        "\n",
        "   We select the feature and threshold that maximize the **reduction** in RSS, computed by subtracting the RSS of the two child nodes from the RSS in the parent node:\n",
        "   $$\n",
        "   (X_j^*, t^*) = \\arg\\max_{X_j, t} \\Bigl\\{\n",
        "   \\underbrace{\\sum_{i : x_i \\in \\text{Parent}} (y_i - \\bar{y})^2}_{\\text{Parent RSS}}\n",
        "   \\;-\\;\n",
        "   \\underbrace{\\sum_{m=1}^{2} \\sum_{i : x_i \\in R_m(X_j, t)} (y_i - \\bar{y}_{R_m})^2}_{\\text{Children RSS}}\n",
        "   \\Bigr\\}.\n",
        "   $$\n",
        "\n",
        "3. **Total RSS Minimization**  \n",
        "   For a dataset $\\{(x_i, y_i)\\}_{i=1}^N$ with features $X$ and target $y$, let $T$ be the current tree.\n",
        "\n",
        "   For any split on feature $X_j$ at threshold $t$, define $T(X_j, t)$ as the new tree obtained by splitting one leaf of $T$ into two leaves $R_1(X_j, t)$ and $R_2(X_j, t)$.\n",
        "   \n",
        "   Let $\\mathrm{Leaves}(T(X_j, t))$ be the set of all leaf indices in this new tree. For each leaf $m \\in \\mathrm{Leaves}(T(X_j, t))$, define:\n",
        "   $$\n",
        "   R_m = \\{\\, i \\,\\mid\\, x_i \\text{ ends in leaf } m\\}.\n",
        "   $$\n",
        "\n",
        "   $R_m$ set collects all data indices $i$ whose feature vector $x_i$ is classified into the leaf node $m$ when passed through the tree $T(X_j,t)$. In other words, each leaf node $m$ in $T(X_j, t)$ corresponds to a unique path of splits, and any data point $x_i$ that follows that path is assigned to the leaf $m$; hence, it belongs to $R_m$.\n",
        "\n",
        "   $R_m$ sets for all leafs $m \\in \\mathrm{Leaves}(T(X_j, t))$ define a partition of all indices.\n",
        "\n",
        "   Then the objective of **minimizing total Residual Sum of Squares (total RSS)** is stated as:\n",
        "   $$\n",
        "   (X_j^*, t^*) = \\arg\\min_{(X_j, t)} \\sum_{m \\in \\mathrm{Leaves}(T(X_j, t))}\n",
        "   \\sum_{i \\in R_m} \\Bigl(y_i - \\overline{y}_{R_m}\\Bigr)^2,\n",
        "   $$\n",
        "   where\n",
        "   $$\n",
        "   \\overline{y}_{R_m} = \\frac{1}{\\lvert R_m \\rvert}\n",
        "   \\sum_{i \\in R_m} y_i\n",
        "   $$\n",
        "   is the mean response in leaf $m$.\n",
        "\n",
        "\n",
        "## **Research Questions**\n",
        "\n",
        "1. **Equivalence Analysis**  \n",
        "   Determine whether the above formulations are equivalent or if they can yield different split choices. Specifically:\n",
        "   - Are *local RSS minimization* and *RSS gain maximization* equivalent?\n",
        "   - Does *total RSS minimization* coincide with either of these two, or is it distinct?\n",
        "   \n",
        "2. **Empirical Experiment**  \n",
        "   Design and conduct a Python experiment to determine which of these formulations is implemented in `scikit-learn` in `DecisionTreeRegressor`. Present numerical results and plots to support your conclusion.\n",
        "\n",
        "\n",
        "## **Tasks & Deliverables**\n",
        "\n",
        "1. **Formulation Analysis**  \n",
        "   - Compare *local RSS minimization*, *RSS gain maximization*, and *total RSS minimization*.\n",
        "   - If you find that any pair of formulations is equivalent, provide a concise proof.  \n",
        "   - If you find that they differ, construct a counterexample.\n",
        "\n",
        "2. **Empirical Verification**  \n",
        "   - Create a small artificial dataset and train a `DecisionTreeRegressor` from `scikit-learn`.\n",
        "   - The dataset must be designed in a way that uniquely identifies the formulation used. Provide a short code snippet and a plot or table to support your conclusion.\n",
        "\n",
        "3. **Report**  \n",
        "   - Summarize your theoretical insights and empirical findings in a **Colab notebook**.\n",
        "   - Include the relevant proofs, code, discussion, and conclusions.\n",
        "   - Place the notebook in your **GitHub repository** for this course, add a link to it in your README.md and add an **“Open in Colab”** badge in the notebook so it can be launched directly.\n",
        "\n"
      ],
      "metadata": {
        "id": "CiiBWLwyz6PI"
      },
      "id": "CiiBWLwyz6PI"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# loading data\n",
        "data = load_breast_cancer()\n",
        "X = data.data[:, :1] # using only the first variable for simplicity and better vizualization\n",
        "y = data.target\n",
        "\n",
        "# splitting dataset into train/test parts\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
        "\n",
        "# calculating regression tree with maximum depth of only 1 in order to examine first split\n",
        "reg_tree = DecisionTreeRegressor(max_depth = 1, random_state = 42)\n",
        "reg_tree.fit(X_train, y_train)\n",
        "sklearn_split = reg_tree.tree_.threshold[0]\n",
        "print(f\"Sklearn split: {sklearn_split}\")  # result = 15.03\n",
        "\n",
        "# 1) Local RSS Minimization\n",
        "def local_rss(X, y, split):\n",
        "    left = y[X.flatten() <= split]\n",
        "    right = y[X.flatten() > split]\n",
        "    rss_left = np.sum((left - np.mean(left))**2) if len(left) > 0 else 0\n",
        "    rss_right = np.sum((right - np.mean(right))**2) if len(right) > 0 else 0\n",
        "    return rss_left + rss_right\n",
        "\n"
      ],
      "metadata": {
        "id": "Kn1L5bBy-Bna",
        "outputId": "70c729cd-8ec9-45a4-9b24-a9933bfbeaf5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Kn1L5bBy-Bna",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sklearn split: 15.025000095367432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sklearn_split"
      ],
      "metadata": {
        "id": "BITeygAd-cah",
        "outputId": "9e969ca4-43c9-4c50-8f03-bdd74d3523c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BITeygAd-cah",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(15.025000095367432)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}