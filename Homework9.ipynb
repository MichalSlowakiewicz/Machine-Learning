{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalSlowakiewicz/Machine-Learning/blob/master/Homework9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Homework Assignment - *Do Androids Dream of Electric Sheep?***\n",
        "\n",
        "-------------------------------------  \n",
        "\n",
        "\"Do Androids Dream of Electric Sheep?\" – the famous title of Philip K. Dick’s novel – raises a fascinating question: if artificial intelligence could dream, what would it see?  \n",
        "\n",
        "In this assignment, we explore a phenomenon known as **neural network dreams**, where instead of optimizing a neural network's weights, we **optimize the input itself** to achieve a desired classification outcome. Given a fully trained MNIST classification network, your goal is to manipulate its inputs so that it confidently predicts each digit from 0 to 9, starting from pure noise.  \n",
        "\n",
        "## **Tasks Description**  \n",
        "\n",
        "During this class we designed and trained a **MNIST classification neural network**, which takes a **batch of grayscale images** of size **$28 \\times 28$** as input and outputs a probability distribution over the 10 digit classes (0–9). However, instead of using real MNIST images, you will **treat the input batch itself as a set of trainable parameters** and optimize it so that the network classifies each image as a specific digit.  \n",
        "\n",
        "1. Your first task is to generate **a batch of 10 images**, where each image is\n",
        "   classified as one of the digits **0, 1, 2, ..., 9**, starting from an initial batch of ten random Gaussian noise images.  \n",
        "\n",
        "   Discuss the following question: do the generated images resemble real MNIST digits? Why or why not?  \n",
        "\n",
        "2. Discuss, how you would approach a second task of\n",
        "   generating an image that   \n",
        "   bares similarity to two or more digits simultaneously. **Implement your idea to see the results.**\n",
        "\n",
        "3. Third task: repeat the previous tasks with an additional L2 penalty on noise within the images. Experiment with adding `lambda_l2 * dreamed_input_batch.pow(2).mean()` loss term, with `lambda_l2` being the penalty cooefficient within an exponential progression, say from 0.001 to 10.0. Are the new digits recognized correctly? How does the penalty impact the digit quality? Explain.\n",
        "\n",
        "### **Optimization Process for Task 1**  \n",
        "\n",
        "1. Start with a **batch of 10 random Gaussian noise images** as the initial input and $(0, 1, 2, \\ldots, 9)$ as the expected output batch of target digits.  \n",
        "2. Define the objective: maximize the neural network's confidence for the corresponding target digit for each image in the batch.  \n",
        "3. Use **gradient descent** to modify the pixels in each image, making the network classify each one as the assigned digit.  \n",
        "4. Repeat until the network assigns suffieciently high confidence to each image’s target class.  \n",
        "\n",
        "### **Implementation Details**  \n",
        "\n",
        "- The neural network weights **must remain frozen** during optimization. You are modifying only the input images.  \n",
        "- The loss function should be the **cross-entropy loss** between the predicted probabilities and the desired class labels (plus an optional weighted L2 penalty regularizing the images in task 3).\n",
        "\n",
        "\n",
        "## **Points to Note**  \n",
        "\n",
        "1. **Visualize** the optimization process: Save images of the generated inputs at different steps and plot the classification confidence evolution over iterations.  \n",
        "3. **Document your findings** and explain the behavior you observe.  \n",
        "\n",
        "## **Task & Deliverables**  \n",
        "\n",
        "- A **Colab notebook** containing solutions for both tasks:\n",
        "  - The full implementation.\n",
        "  - Visualizations of the generated batch of images.\n",
        "  - A written explanation of your observations.\n",
        "- **Bonus:** If you create an **animation** showing the evolution of the input images during optimization, it will be considered a strong enhancement to your submission.\n",
        "  - You can generate an animation programmatically (e.g., using Matplotlib or OpenCV).\n",
        "  - Or, save image frames and use external tools to create a video.\n",
        "  - Provide a **link** to any video files in the README.\n",
        "- Upload your notebook and results to your **GitHub repository** for the course.\n",
        "- In the **README**, include a **link** to the notebook.\n",
        "- In the notebook, include **“Open in Colab”** badge so it can be launched directly.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZJ5c7P6ChB3e"
      },
      "id": "ZJ5c7P6ChB3e"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}